{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    max_epochs: int = 100\n",
    "    patience: int = 10\n",
    "    grad_clip: float = 1.0\n",
    "    noise_std: float = 0.05\n",
    "    hidden_size: int = 256\n",
    "    train_split: float = 0.8\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    \n",
    "class ODEDynamics(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.LayerNorm(hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size//2, 3)\n",
    "        )\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.should_stop\n",
    "\n",
    "class ODETrainer:\n",
    "    def __init__(self, config: TrainingConfig, device: torch.device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.writer = SummaryWriter(log_dir=\"runs/ODEExperiment\")\n",
    "        os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "    def generate_data(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        t = torch.linspace(0, 10, 1000).to(self.device)\n",
    "        true_y = torch.zeros(1000, 3).to(self.device)\n",
    "        \n",
    "        # More complex dynamics\n",
    "        true_y[:, 0] = torch.sin(t) + 0.5 * torch.sin(2*t) + self.config.noise_std * torch.randn_like(t)\n",
    "        true_y[:, 1] = torch.cos(t) + 0.5 * torch.cos(3*t) + self.config.noise_std * torch.randn_like(t)\n",
    "        true_y[:, 2] = 0.5 * torch.sin(t) * torch.cos(t) + self.config.noise_std * torch.randn_like(t)\n",
    "        \n",
    "        return t, true_y\n",
    "\n",
    "    def prepare_dataloaders(self, t: torch.Tensor, true_y: torch.Tensor):\n",
    "        train_size = int(self.config.train_split * len(t))\n",
    "        train_t, val_t = torch.split(t, [train_size, len(t) - train_size])\n",
    "        train_y, val_y = torch.split(true_y, [train_size, len(t) - train_size])\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(train_t, train_y)\n",
    "        val_dataset = torch.utils.data.TensorDataset(val_t, val_y)\n",
    "        \n",
    "        return (\n",
    "            torch.utils.data.DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True),\n",
    "            torch.utils.data.DataLoader(val_dataset, batch_size=self.config.batch_size)\n",
    "        )\n",
    "\n",
    "    def train_model(self, solver: str):\n",
    "        model = ODEDynamics(self.config.hidden_size).to(self.device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        early_stopping = EarlyStopping(patience=self.config.patience)\n",
    "        \n",
    "        t, true_y = self.generate_data()\n",
    "        train_loader, val_loader = self.prepare_dataloaders(t, true_y)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            train_loss = self._train_epoch(model, train_loader, optimizer, loss_fn, solver)\n",
    "            val_loss = self._validate_epoch(model, val_loader, loss_fn, solver)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self._save_checkpoint(model, optimizer, epoch, val_loss, solver)\n",
    "            \n",
    "            self._log_metrics(train_loss, val_loss, model, true_y, t, solver, epoch)\n",
    "            \n",
    "            if early_stopping(val_loss):\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "                \n",
    "        return model, best_val_loss\n",
    "\n",
    "    def _train_epoch(self, model, train_loader, optimizer, loss_fn, solver) -> float:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for t_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = odeint(model, y_batch[0], t_batch, method=solver).to(self.device)\n",
    "            loss = loss_fn(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def _validate_epoch(self, model, val_loader, loss_fn, solver) -> float:\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t_val, y_val in val_loader:\n",
    "                pred_val = odeint(model, y_val[0], t_val, method=solver).to(self.device)\n",
    "                loss = loss_fn(pred_val, y_val)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def _save_checkpoint(self, model, optimizer, epoch, val_loss, solver):\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_loss': val_loss\n",
    "        }\n",
    "        path = os.path.join(self.config.checkpoint_dir, f'model_{solver}_best.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    def _log_metrics(self, train_loss, val_loss, model, true_y, t, solver, epoch):\n",
    "        self.writer.add_scalar(f\"{solver}/Train Loss\", train_loss, epoch)\n",
    "        self.writer.add_scalar(f\"{solver}/Validation Loss\", val_loss, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            self._visualize_predictions(model, true_y, t, solver, epoch)\n",
    "\n",
    "    def _visualize_predictions(self, model, true_y, t, solver, epoch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(model, true_y[0], t, method=solver)\n",
    "            \n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot(true_y[:, 0].cpu(), true_y[:, 1].cpu(), true_y[:, 2].cpu(), \n",
    "                label='True', linewidth=2, alpha=0.8)\n",
    "        ax.plot(pred_y[:, 0].cpu(), pred_y[:, 1].cpu(), pred_y[:, 2].cpu(), \n",
    "                label='Predicted', linewidth=2, alpha=0.8)\n",
    "        ax.set_title(f\"{solver} - Epoch {epoch}\")\n",
    "        ax.legend()\n",
    "        self.writer.add_figure(f\"{solver}/Trajectories\", fig, epoch)\n",
    "        plt.close(fig)\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config = TrainingConfig()\n",
    "    trainer = ODETrainer(config, device)\n",
    "    \n",
    "    solvers = ['dopri5', 'rk4', 'implicit_adams']\n",
    "    results = {}\n",
    "    \n",
    "    for solver in solvers:\n",
    "        print(f\"\\nTraining with {solver} solver...\")\n",
    "        model, val_loss = trainer.train_model(solver)\n",
    "        results[solver] = {'model': model, 'val_loss': val_loss}\n",
    "        print(f\"Best validation loss with {solver}: {val_loss:.6f}\")\n",
    "    \n",
    "    trainer.writer.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
